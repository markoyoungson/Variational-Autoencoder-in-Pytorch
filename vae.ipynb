{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torch.optim as optim \n",
    "import argparse\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from PIL import *\n",
    "import imageio\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "source": [
    "kernel_size = 4 # (4, 4) kernel\n",
    "image_channels = 1 # MNIST images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling\n",
    "        "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vid(images): \n",
    "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
    "    imageio.mimsave(f'outputs/generated_images.gif',imgs)\n",
    "def save_reconstructed_images(recon_images, epoch):\n",
    "    save_image(recon_images.cpu(), f'outputs/output{epoch}.jpg')\n",
    "def save_loss_plot(train_loss, valid_loss):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(train_loss, color='red', label='train loss')\n",
    "    plt.plot(valid_loss, color='blue', label='validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('outputs/loss.jpg')\n",
    "    plt.show()\n",
    "to_pil_image = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Conv2d(in_channels=1,out_channels=8,kernel_size=(4,4),stride=2, padding=1)\n",
    "        self.enc2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(4,4), stride=2, padding=1)\n",
    "        self.enc3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(4,4), stride=2, padding=1)\n",
    "        self.enc4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(4,4),stride=2, padding=0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc_mu = nn.Linear(128,16)\n",
    "        self.fc_var = nn.Linear(128,16)\n",
    "        self.fc2 = nn.Linear(latent_dim, 64)\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,kernel_size=(4,4), stride=2,padding=0)\n",
    "        self.dec2 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=(4,4), stride=2, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=(4,4), stride=2, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=(4,4), stride=2, padding=1)\n",
    "\n",
    "\n",
    "        # Reparameterization Trick\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var) \n",
    "        eps = torch.randn_like(std) \n",
    "        sample = mu + (eps * std)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        \n",
    "        batch, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(batch,-1)\n",
    "        hidden = self.fc1(x)\n",
    "\n",
    "        mu = self.fc_mu(hidden)\n",
    "        log_var = self.fc_var(hidden)\n",
    "          \n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        z = self.fc2(z)\n",
    "\n",
    "        z = z.view(-1,64,1,1)\n",
    "\n",
    "        # Decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        reconstruction = torch.sigmoid(self.dec4(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ConvVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "grid_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set and train data loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(), ])\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='../input', train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "# validation set and validation data loader\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='../input', train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss, mu, logvar):\n",
    "\n",
    "    MSE = mse_loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "        counter +=1\n",
    "        data = data[0]\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        mse_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(mse_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = running_loss/ counter\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "            counter += 1\n",
    "            data = data[0]\n",
    "            data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
    "                recon_images = reconstruction\n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(\n",
    "        model, trainloader, trainset, device, optimizer, criterion\n",
    "    )\n",
    "    valid_epoch_loss, recon_images = validate(\n",
    "        model, testloader, testset, device, criterion\n",
    "    )\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    # save the reconstructed images from the validation loop\n",
    "    save_reconstructed_images(recon_images, epoch+1)\n",
    "    # convert the reconstructed images to PyTorch image grid format\n",
    "    image_grid = make_grid(recon_images.detach().cpu())\n",
    "    grid_images.append(image_grid)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {valid_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the reconstructions as a .gif file\n",
    "image_to_vid(grid_images)\n",
    "# save the loss plots to disk\n",
    "save_loss_plot(train_loss, valid_loss)\n",
    "print('TRAINING COMPLETE')"
   ]
  }
 ]
}